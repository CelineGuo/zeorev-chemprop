{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##new data (zeodb) for later use\n",
    "df_zeodb_af = pd.read_csv('.../zeodb_affinity_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns: ['Code', 'deriv_dist_46', 'deriv_dist_57', 'cum_skewness', 'deriv_dist_55', 'deriv_dist_67', 'deriv_dist_74', 'ring_size_2', 'deriv_dist_53', 'ring_size_3', 'volume', 'a', 'poc_2_sa_a2', 'c', 'num_atoms', 'chan_1_vol_a3', 'deriv_dist_31', 'unitcell_vol', 'deriv_dist_38', 'chan_0_di', 'b', 'deriv_dist_72', 'beta', 'cum_variance', 'ponav_frac', 'cum_kurtosis', 'deriv_dist_35', 'deriv_dist_34', 'chan_0_sa_a2', 'largest_free_sphere_izc', 'deriv_dist_50', 'deriv_dist_56', 'ovlpvfract', 'td_10', 'deriv_dist_42', 'deriv_dist_43', 'nav_cm3_g']\n",
      "Number of rows before cleaning: 30164\n",
      "Number of rows after cleaning: 23961\n",
      "Columns before cleaning: 37\n"
     ]
    }
   ],
   "source": [
    "df_ori = pd.read_excel('.../ZEOSYN.xlsx')\n",
    "df_zeos_reduced = pd.read_csv('.../data_reduced_1.csv')\n",
    "df_zeos_reduced.fillna('0.0', inplace=True)\n",
    "mask_numeric = df_zeos_reduced.apply(\n",
    "    lambda col: pd.to_numeric(col, errors='coerce').notna().all()\n",
    ")\n",
    "mask_numeric['Code'] = True\n",
    "\n",
    "df_zeos_reduced = df_zeos_reduced.loc[:, mask_numeric]\n",
    "print(\"Remaining columns:\", df_zeos_reduced.columns.tolist())\n",
    "\n",
    "rows_before_cleaning = df_ori.shape[0]\n",
    "df = df_ori.dropna(subset=['Si'],how=\"all\")\n",
    "rows_after_cleaning = df.shape[0]\n",
    "print(f\"Number of rows before cleaning: {rows_before_cleaning}\")\n",
    "print(f\"Number of rows after cleaning: {rows_after_cleaning}\")\n",
    "print(f\"Columns before cleaning: {df_zeos_reduced.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match zeolite code with zeolite descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned unmatched zeolites: ['AEI', 'AFX', 'ASU-12', 'ASU-14', 'ASU-16', 'BEA', 'CHA', 'CLO', 'CTH', 'ERI', 'EUO', 'FAU', 'FDU-4', 'IFT', 'IFU', 'IM-14', 'IRY', 'ISV', 'ITN', 'ITQ-21', 'ITV', 'LIT', 'MEL', 'MFI', 'MOR', 'MRE', 'NUD-1', 'PKU-17', 'RUT', 'SFV', 'STF', 'STO', 'SU-65', 'SU-67', 'SU-74', 'SU-77', 'SU-79', 'SU-M', 'SU-MB', 'SVR', 'SVY', 'SYSU-3', 'TON', 'UOE']\n",
      "Cleaned length: 44\n",
      "Remaining unmatched zeolites after cleaning: ['ASU-12', 'ASU-14', 'ASU-16', 'FDU-4', 'IM-14', 'ITQ-21', 'NUD-1', 'PKU-17', 'SU-65', 'SU-67', 'SU-74', 'SU-77', 'SU-79', 'SU-M', 'SU-MB', 'SYSU-3']\n",
      "Remaining unmatched count: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42684/3784235628.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].apply(lambda x: clean_zeolite_name(x) if pd.notna(x) and x in zeo_not_in_descriptors else x)\n"
     ]
    }
   ],
   "source": [
    "zeo_codes = set(df_zeos_reduced[\"Code\"].dropna().unique())\n",
    "\n",
    "all_zeolites = df[[\"Code1\", \"Code2\", \"Code3\"]].values.flatten()\n",
    "all_zeolites = {z for z in all_zeolites if pd.notna(z)}\n",
    "\n",
    "# Find zeolites NOT in df_zeos_reduced[\"Code\"] \n",
    "zeo_not_in_descriptors = all_zeolites - zeo_codes\n",
    "\n",
    "# Function to clean zeolite names by removing leading special characters and slashes\n",
    "def clean_zeolite_name(name):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    name = re.sub(r'^[-*]+', '', name)  # Remove leading symbols (*, -, etc.)\n",
    "    name = name.split('/')[0].strip()  # Keep only the first part if separated by '/'\n",
    "    return name\n",
    "\n",
    "# Apply cleaning to unmatched zeolites\n",
    "cleaned_zeo_not_in_descriptors = {clean_zeolite_name(z) for z in zeo_not_in_descriptors}\n",
    "\n",
    "# Update dataset: Replace old values with cleaned values\n",
    "for col in [\"Code1\", \"Code2\", \"Code3\"]:\n",
    "    df[col] = df[col].apply(lambda x: clean_zeolite_name(x) if pd.notna(x) and x in zeo_not_in_descriptors else x)\n",
    "\n",
    "# Recalculate unmatched zeolites after cleaning\n",
    "all_zeolites_updated = df[[\"Code1\", \"Code2\", \"Code3\"]].values.flatten()\n",
    "all_zeolites_updated = {z for z in all_zeolites_updated if pd.notna(z)}\n",
    "zeo_still_not_in_descriptors = all_zeolites_updated - zeo_codes\n",
    "\n",
    "print(\"Cleaned unmatched zeolites:\", sorted(cleaned_zeo_not_in_descriptors))\n",
    "print(\"Cleaned length:\", len(cleaned_zeo_not_in_descriptors))\n",
    "print(\"Remaining unmatched zeolites after cleaning:\", sorted(zeo_still_not_in_descriptors))\n",
    "print(\"Remaining unmatched count:\", len(zeo_still_not_in_descriptors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. synthesis routes with zeolite features: 18378\n",
      "Num. synthesis routes without products: 5371\n",
      "Num. total synthesis routes: 18378\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code1</th>\n",
       "      <th>Code2</th>\n",
       "      <th>Code3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BPH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30015</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30016</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30026</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30084</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30112</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23749 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Code1 Code2 Code3\n",
       "0       AFI   NaN   NaN\n",
       "1       AFI   NaN   NaN\n",
       "2       AFI   NaN   NaN\n",
       "3       AFI   NaN   NaN\n",
       "5       BPH   NaN   NaN\n",
       "...     ...   ...   ...\n",
       "30015   NaN   NaN   NaN\n",
       "30016   NaN   NaN   NaN\n",
       "30026   NaN   NaN   NaN\n",
       "30084   NaN   NaN   NaN\n",
       "30112   NaN   NaN   NaN\n",
       "\n",
       "[23749 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_zeo_feats = df[\n",
    "    df[[\"Code1\", \"Code2\", \"Code3\"]].apply(\n",
    "        lambda row: any(z in zeo_codes for z in row if pd.notna(z)) and\n",
    "                    all(z not in zeo_still_not_in_descriptors for z in row if pd.notna(z)), \n",
    "        axis=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print('Num. synthesis routes with zeolite features:', len(df_with_zeo_feats))\n",
    "\n",
    "#Add zeolite-free syntheses\n",
    "df_zeo_free = df[\n",
    "    df['Code1'].isna() & df['Code2'].isna() & df['Code3'].isna()\n",
    "]\n",
    "\n",
    "df_all_zeo_feats = pd.concat([df_with_zeo_feats, df_zeo_free])\n",
    "print('Num. synthesis routes without products:', len(df_zeo_free))\n",
    "print('Num. total synthesis routes:', len(df_with_zeo_feats))\n",
    "\n",
    "df_all_zeo_feats[[\"Code1\", \"Code2\", \"Code3\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yield counts:\n",
      "0.00     5371\n",
      "0.33      504\n",
      "0.50     4106\n",
      "1.00    16157\n",
      "Name: yield, dtype: int64\n",
      "                                        osda1_smiles  osda2_smiles  \\\n",
      "0  C[N+]12CCCC[C@@H]1[C@H]1C[C@@H](C2)[C@@H]2CCCC...           NaN   \n",
      "1  C[N+]12CCCC[C@@H]1[C@H]1C[C@@H](C2)[C@@H]2CCCC...           NaN   \n",
      "2                    C[N+](C)(C)C12CC3CC(CC(C3)C1)C2           NaN   \n",
      "3                    C[N+](C)(C)C12CC3CC(CC(C3)C1)C2           NaN   \n",
      "4                                   CC[N+](CC)(CC)CC  C[N+](C)(C)C   \n",
      "\n",
      "  osda3_smiles zeolite_code  yield  cryst_time  cryst_temp  \n",
      "0          NaN          AFI    1.0       504.0       150.0  \n",
      "1          NaN          AFI    1.0       168.0       175.0  \n",
      "2          NaN          AFI    1.0       144.0       150.0  \n",
      "3          NaN          AFI    1.0         NaN         NaN  \n",
      "4          NaN          BPH    1.0        72.0       125.0  \n"
     ]
    }
   ],
   "source": [
    "def calculate_yield(row):\n",
    "    count_non_nan = row.notna().sum()\n",
    "    if count_non_nan > 0:\n",
    "        return 1/count_non_nan\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def augment_data_correct_gel_composition(df_all_zeo_feats):\n",
    "    augmented_rows = []\n",
    "    \n",
    "    # gel_composition_columns = df_all_zeo_feats.loc[:, 'Si':'OH'].columns\n",
    "    all_zeolite_codes = df_all_zeo_feats[['Code1', 'Code2', 'Code3']].stack().dropna().unique()\n",
    "    \n",
    "    for index, row in df_all_zeo_feats.iterrows():\n",
    "        # gel_composition = row[gel_composition_columns]\n",
    "        osda_smiles = row[['osda1 smiles', 'osda2 smiles', 'osda3 smiles']]\n",
    "        zeolite_codes = [row['Code1'], row['Code2'], row['Code3']]\n",
    "        cryst_time = row['cryst_time']\n",
    "        cryst_temp = row['cryst_temp']\n",
    "        \n",
    "        # Calculate the yield based on non-NaN values in the zeolite codes\n",
    "        calculated_yield = calculate_yield(pd.Series(zeolite_codes))\n",
    "            \n",
    "        # Ensure every row contributes, even if all zeolite codes are NaN\n",
    "        if all(pd.isnull(zeolite_codes)):\n",
    "            valid_zeolite_codes = [random.choice(all_zeolite_codes)]  # Assign a random zeolite\n",
    "            calculated_yield = 0\n",
    "        else:\n",
    "            # Process and filter zeolite codes, cleaning symbols\n",
    "            valid_zeolite_codes = [code for code in zeolite_codes if pd.notnull(code)]\n",
    "        \n",
    "        for zeolite_code in valid_zeolite_codes:\n",
    "            augmented_row = osda_smiles.tolist() + [zeolite_code, calculated_yield, cryst_time, cryst_temp]\n",
    "            augmented_rows.append(augmented_row)\n",
    "    \n",
    "    augmented_columns = ['osda1_smiles', 'osda2_smiles', 'osda3_smiles', 'zeolite_code', 'yield', 'cryst_time', 'cryst_temp']\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows, columns=augmented_columns)\n",
    "    yield_counts = augmented_df['yield'].round(2).value_counts().sort_index()\n",
    "    print(\"\\nYield counts:\")\n",
    "    print(yield_counts)\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "augmented_df_gel_correct = augment_data_correct_gel_composition(df_all_zeo_feats)\n",
    "print(augmented_df_gel_correct.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>osda1_smiles</th>\n",
       "      <th>osda2_smiles</th>\n",
       "      <th>osda3_smiles</th>\n",
       "      <th>zeolite_code</th>\n",
       "      <th>yield</th>\n",
       "      <th>cryst_time</th>\n",
       "      <th>cryst_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C[N+]12CCCC[C@@H]1[C@H]1C[C@@H](C2)[C@@H]2CCCC...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AFI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[N+]12CCCC[C@@H]1[C@H]1C[C@@H](C2)[C@@H]2CCCC...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AFI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C[N+](C)(C)C12CC3CC(CC(C3)C1)C2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AFI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC[N+](CC)(CC)CC</td>\n",
       "      <td>C[N+](C)(C)C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BPH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CC[N+](CC)(CC)CC</td>\n",
       "      <td>C[N+](C)(C)C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UFI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        osda1_smiles  osda2_smiles  \\\n",
       "0  C[N+]12CCCC[C@@H]1[C@H]1C[C@@H](C2)[C@@H]2CCCC...           NaN   \n",
       "1  C[N+]12CCCC[C@@H]1[C@H]1C[C@@H](C2)[C@@H]2CCCC...           NaN   \n",
       "2                    C[N+](C)(C)C12CC3CC(CC(C3)C1)C2           NaN   \n",
       "4                                   CC[N+](CC)(CC)CC  C[N+](C)(C)C   \n",
       "5                                   CC[N+](CC)(CC)CC  C[N+](C)(C)C   \n",
       "\n",
       "  osda3_smiles zeolite_code  yield  cryst_time  cryst_temp  \n",
       "0          NaN          AFI    1.0       504.0       150.0  \n",
       "1          NaN          AFI    1.0       168.0       175.0  \n",
       "2          NaN          AFI    1.0       144.0       150.0  \n",
       "4          NaN          BPH    1.0        72.0       125.0  \n",
       "5          NaN          UFI    1.0        96.0       150.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_osda = augmented_df_gel_correct[\n",
    "    augmented_df_gel_correct[['osda1_smiles','osda2_smiles','osda3_smiles']].notna().any(axis=1)\n",
    "]\n",
    "\n",
    "##if temperature OR time is missing, drop that row\n",
    "df_with_osda = df_with_osda[\n",
    "    df_with_osda['cryst_time'].notna() & df_with_osda['cryst_temp'].notna()\n",
    "]\n",
    "df_with_osda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCC[N+]12CCN(CC1)CC2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCC[N+]12CCN(CC1)CC2}' for input: '{CCC[N+]12CCN(CC1)CC2}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCC[N+]12CCN(CC1)CC2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCC[N+]12CCN(CC1)CC2}' for input: '{CCC[N+]12CCN(CC1)CC2}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCC[N+]12CCN(CC1)CC2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCC[N+]12CCN(CC1)CC2}' for input: '{CCC[N+]12CCN(CC1)CC2}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCC[N+]12CCN(CC1)CC2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCC[N+]12CCN(CC1)CC2}' for input: '{CCC[N+]12CCN(CC1)CC2}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCC[N+]12CCN(CC1)CC2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCC[N+]12CCN(CC1)CC2}' for input: '{CCC[N+]12CCN(CC1)CC2}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCC[N+]12CCN(CC1)CC2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCC[N+]12CCN(CC1)CC2}' for input: '{CCC[N+]12CCN(CC1)CC2}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCC[N+]12CCN(CC1)CC2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCC[N+]12CCN(CC1)CC2}' for input: '{CCC[N+]12CCN(CC1)CC2}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCCC[N+]12CCN(CC1)CC2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCCC[N+]12CCN(CC1)CC2}' for input: '{CCCC[N+]12CCN(CC1)CC2}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCCC[N+]12CCN(CC1)CC2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCCC[N+]12CCN(CC1)CC2}' for input: '{CCCC[N+]12CCN(CC1)CC2}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCCC[N+]12CCN(CC1)CC2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCCC[N+]12CCN(CC1)CC2}' for input: '{CCCC[N+]12CCN(CC1)CC2}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCCC[N+]12CCN(CC1)CC2C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCCC[N+]12CCN(CC1)CC2C}' for input: '{CCCC[N+]12CCN(CC1)CC2C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCCC[N+]12CCN(CC1)CC2C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCCC[N+]12CCN(CC1)CC2C}' for input: '{CCCC[N+]12CCN(CC1)CC2C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCCC[N+]12CCN(CC1)CC2C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCCC[N+]12CCN(CC1)CC2C}' for input: '{CCCC[N+]12CCN(CC1)CC2C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCCC[N+]12CCN(CC1C)C(C)C2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCCC[N+]12CCN(CC1C)C(C)C2}' for input: '{CCCC[N+]12CCN(CC1C)C(C)C2}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCCC[N+]12CCN(CC1C)C(C)C2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCCC[N+]12CCN(CC1C)C(C)C2}' for input: '{CCCC[N+]12CCN(CC1C)C(C)C2}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCCC[N+]12CCN(CC1C)C(C)C2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCCC[N+]12CCN(CC1C)C(C)C2}' for input: '{CCCC[N+]12CCN(CC1C)C(C)C2}'\n",
      "[09:50:20] WARNING: not removing hydrogen atom without neighbors\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CNC}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CNC}' for input: '{CNC}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCCC[N+]12CCN(CC1C)C(C)C2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCCC[N+]12CCN(CC1C)C(C)C2}' for input: '{CCCC[N+]12CCN(CC1C)C(C)C2}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CCCC[N+]12CCN(CC1C)C(C)C2}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CCCC[N+]12CCN(CC1C)C(C)C2}' for input: '{CCCC[N+]12CCN(CC1C)C(C)C2}'\n",
      "[09:50:20] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:20] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n",
      "[09:50:21] SMILES Parse Error: syntax error while parsing: {CC1C[N+](C)(C)CC1C}\n",
      "[09:50:21] SMILES Parse Error: Failed parsing SMILES '{CC1C[N+](C)(C)CC1C}' for input: '{CC1C[N+](C)(C)CC1C}'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_valid 792\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "def is_valid_smiles_or_empty(smiles):\n",
    "    if pd.isna(smiles):\n",
    "        return True  # Consider empty, NaN, or '0' as v\n",
    "    return Chem.MolFromSmiles(smiles) is not None\n",
    "\n",
    "df_valid = df_with_osda[\n",
    "    df_with_osda['osda1_smiles'].apply(is_valid_smiles_or_empty) &\n",
    "    df_with_osda['osda2_smiles'].apply(is_valid_smiles_or_empty) &\n",
    "    df_with_osda['osda3_smiles'].apply(is_valid_smiles_or_empty)\n",
    "]\n",
    "\n",
    "unique_osdas_valid = set(df_valid['osda1_smiles'].value_counts().keys()) |\\\n",
    "                        set(df_valid['osda2_smiles'].value_counts().keys()) |\\\n",
    "                        set(df_valid['osda3_smiles'].value_counts().keys())\n",
    "\n",
    "print(\"df_valid\", len(unique_osdas_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Combined:  (21184, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42684/3692829824.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_combined['osda_combined'] = new_combined[['osda1_smiles', 'osda2_smiles', 'osda3_smiles']].fillna(\"\").agg('.'.join, axis=1)\n",
      "/tmp/ipykernel_42684/3692829824.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_combined['osda_combined'] = new_combined['osda_combined'].str.replace(r'\\.+', '.', regex=True).str.strip('.')\n",
      "/tmp/ipykernel_42684/3692829824.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_combined.drop(columns=['osda1_smiles', 'osda2_smiles', 'osda3_smiles'], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zeolite_code</th>\n",
       "      <th>osda_combined</th>\n",
       "      <th>yield</th>\n",
       "      <th>cryst_time</th>\n",
       "      <th>cryst_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFI</td>\n",
       "      <td>C[N+]12CCCC[C@@H]1[C@H]1C[C@@H](C2)[C@@H]2CCCC...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFI</td>\n",
       "      <td>C[N+]12CCCC[C@@H]1[C@H]1C[C@@H](C2)[C@@H]2CCCC...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFI</td>\n",
       "      <td>C[N+](C)(C)C12CC3CC(CC(C3)C1)C2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BPH</td>\n",
       "      <td>CC[N+](CC)(CC)CC.C[N+](C)(C)C</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UFI</td>\n",
       "      <td>CC[N+](CC)(CC)CC.C[N+](C)(C)C</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  zeolite_code                                      osda_combined  yield  \\\n",
       "0          AFI  C[N+]12CCCC[C@@H]1[C@H]1C[C@@H](C2)[C@@H]2CCCC...    1.0   \n",
       "1          AFI  C[N+]12CCCC[C@@H]1[C@H]1C[C@@H](C2)[C@@H]2CCCC...    1.0   \n",
       "2          AFI                    C[N+](C)(C)C12CC3CC(CC(C3)C1)C2    1.0   \n",
       "4          BPH                      CC[N+](CC)(CC)CC.C[N+](C)(C)C    1.0   \n",
       "5          UFI                      CC[N+](CC)(CC)CC.C[N+](C)(C)C    1.0   \n",
       "\n",
       "   cryst_time  cryst_temp  \n",
       "0       504.0       150.0  \n",
       "1       168.0       175.0  \n",
       "2       144.0       150.0  \n",
       "4        72.0       125.0  \n",
       "5        96.0       150.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_extract = [\"osda1_smiles\", \"osda2_smiles\", \"osda3_smiles\", \"zeolite_code\"]\n",
    "#gel_comp = df_valid[['Si', 'Al', 'H2O','sda1', 'sda2', 'sda3', 'OH']].copy()\n",
    "# gel_comp = df_valid.loc[:, 'Si':'OH'].copy()\n",
    "new_combined = df_valid[columns_to_extract]\n",
    "new_combined['osda_combined'] = new_combined[['osda1_smiles', 'osda2_smiles', 'osda3_smiles']].fillna(\"\").agg('.'.join, axis=1)\n",
    "new_combined['osda_combined'] = new_combined['osda_combined'].str.replace(r'\\.+', '.', regex=True).str.strip('.')\n",
    "new_combined.drop(columns=['osda1_smiles', 'osda2_smiles', 'osda3_smiles'], inplace=True)\n",
    "new_combined = pd.concat([new_combined, df_valid.drop(columns=columns_to_extract)], axis=1)\n",
    "# new_combined = pd.concat([new_combined, df_valid.drop(columns=columns_to_extract)], axis=1)\n",
    "# new_combined.drop(columns=['zeolite_code'], inplace=True)\n",
    "print(\"New Combined: \", new_combined.shape)\n",
    "new_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    16434\n",
      "0     4750\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def assign_class(yield_value):\n",
    "    if pd.isna(yield_value):\n",
    "        return 0\n",
    "    elif yield_value == 1:\n",
    "        return 1\n",
    "    elif yield_value == 1/2:\n",
    "        return 1\n",
    "    elif yield_value == 0.333333333:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "new_combined['class'] = new_combined['yield'].apply(assign_class)\n",
    "new_combined.drop(columns=['yield'], inplace=True)\n",
    "print(new_combined['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##if one row has multiple osdas separated by dot (.) --> split it\n",
    "def augment_osda_pairs(df):\n",
    "    records = []  \n",
    "    for _, row in df.iterrows():\n",
    "        osdas = row[\"osda_combined\"].split(\".\")\n",
    "        for osda in osdas:\n",
    "            rec = {\n",
    "                \"osda\": osda.strip(),\n",
    "                \"class\": row[\"class\"],\n",
    "                \"zeolite_code\": row[\"zeolite_code\"]\n",
    "            }\n",
    "            records.append(rec)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "new_combined2 = augment_osda_pairs(new_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_combined.drop_duplicates(subset=['osda_combined','class','zeolite_code','cryst_temp','cryst_time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for adding zeodb data --> if you don't want to add, then skip\n",
    "from_zeodb = df_zeodb_af[['Host', 'SMILES']]\n",
    "\n",
    "from_zeodb['class'] = 1.0\n",
    "from_zeodb.columns.tolist()\n",
    "\n",
    "new_combined_3 = pd.concat([new_combined2, from_zeodb.rename(columns={'Host': 'zeolite_code', 'SMILES': 'osda', 'class':'class'})], ignore_index=True)\n",
    "\n",
    "#drop duplicates if any (same OSDA, same zeolite)\n",
    "new_combined_3.drop_duplicates(subset=['osda', 'zeolite_code'], inplace=True)\n",
    "print(\"Final dataset shape:\", new_combined_3.shape)\n",
    "\n",
    "new_combined_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_combined_3.to_csv('.../all_data_ori.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv for false data generation (refer to to artificial_data_gen.py)\n",
    "new_combined_3.to_csv(\".../want_to_gen.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#take the file from artificial_data_gen\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m new_combined_gen \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.../after_gen.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m new_combined_gen \u001b[38;5;241m=\u001b[39m new_combined_gen\u001b[38;5;241m.\u001b[39mmerge(df_zeos_reduced, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeolite_code\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#take the file from artificial_data_gen\n",
    "\n",
    "new_combined_gen = pd.read_csv(\".../after_gen.csv\")\n",
    "new_combined_gen = new_combined_gen.merge(df_zeos_reduced, left_on='zeolite_code', right_on='Code', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only take the rows with class = 1.0\n",
    "new_combined_gen = new_combined[new_combined['class'] == 1.0]\n",
    "new_combined_gen = new_combined_gen.merge(df_zeos_reduced, left_on='zeolite_code', right_on='Code', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing zeolite descriptors\n",
    "\n",
    "minmax = MinMaxScaler()\n",
    "desc_start = 'deriv_dist_46'\n",
    "desc_end = 'nav_cm3_g'\n",
    "\n",
    "desc_cols = new_combined_gen.loc[:, desc_start:desc_end].columns\n",
    "new_combined_gen[desc_cols] = minmax.fit_transform(new_combined_gen[desc_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution in Training Data:\n",
      "1    6090\n",
      "dtype: int64\n",
      "Class Distribution in Validation Data:\n",
      "1    338\n",
      "dtype: int64\n",
      "Class Distribution in Test Data:\n",
      "1    339\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def stratified_split(data, labels, train_size=0.9, val_size=0.05, test_size=0.05, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits data into train, validation, and test sets with stratified sampling.\n",
    "    Shuffles within each split while keeping rows intact.\n",
    "    \"\"\"\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 1e-8, \"Train, val, and test sizes must sum to 1.0\"\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "\n",
    "    # Separate data by class\n",
    "    data_by_class = {cls: data[labels == cls] for cls in labels.unique()}\n",
    "    \n",
    "    train_data, train_labels = [], []\n",
    "    val_data, val_labels = [], []\n",
    "    test_data, test_labels = [], []\n",
    "    \n",
    "    for cls, cls_data in data_by_class.items():\n",
    "        cls_train_size = train_size\n",
    "        cls_val_size = val_size\n",
    "        cls_test_size = test_size\n",
    "        cls_test_adjusted = 1 - cls_train_size\n",
    "            \n",
    "        cls_train, cls_remaining = train_test_split(\n",
    "            cls_data, test_size=cls_test_adjusted, random_state=random_state\n",
    "        )\n",
    "        cls_val, cls_test = train_test_split(\n",
    "            cls_remaining, test_size=cls_test_size / (cls_val_size + cls_test_size), random_state=random_state\n",
    "        )\n",
    "            \n",
    "        train_data.append(cls_train)\n",
    "        val_data.append(cls_val)\n",
    "        test_data.append(cls_test)\n",
    "        train_labels += [cls] * len(cls_train)\n",
    "        val_labels += [cls] * len(cls_val)\n",
    "        test_labels += [cls] * len(cls_test)\n",
    "    \n",
    "    # Concatenate splits\n",
    "    train_data = pd.concat(train_data)\n",
    "    val_data = pd.concat(val_data)\n",
    "    test_data = pd.concat(test_data)\n",
    "\n",
    "    # Shuffle each set but keep rows intact\n",
    "    train_data, train_labels = train_data.sample(frac=1, random_state=random_state), pd.Series(train_labels, index=train_data.index)\n",
    "    val_data, val_labels = val_data.sample(frac=1, random_state=random_state), pd.Series(val_labels, index=val_data.index)\n",
    "    test_data, test_labels = test_data.sample(frac=1, random_state=random_state), pd.Series(test_labels, index=test_data.index)\n",
    "\n",
    "    # Reset indices to keep alignment\n",
    "    train_data, train_labels = train_data.reset_index(drop=True), train_labels.reset_index(drop=True)\n",
    "    val_data, val_labels = val_data.reset_index(drop=True), val_labels.reset_index(drop=True)\n",
    "    test_data, test_labels = test_data.reset_index(drop=True), test_labels.reset_index(drop=True)\n",
    "    \n",
    "    return train_data, val_data, test_data, train_labels, val_labels, test_labels\n",
    "\n",
    "train_data, val_data, test_data, train_labels, val_labels, test_labels = stratified_split(new_combined_gen, new_combined_gen['class'])\n",
    "\n",
    "print(\"Class Distribution in Training Data:\")\n",
    "print(pd.Series(train_labels).value_counts(normalize=False))\n",
    "print(\"Class Distribution in Validation Data:\")\n",
    "print(pd.Series(val_labels).value_counts(normalize=False))\n",
    "print(\"Class Distribution in Test Data:\")\n",
    "print(pd.Series(test_labels).value_counts(normalize=False))\n",
    "\n",
    "##use below code if you want to split train and val only (after separating test set manually)\n",
    "# def stratified_train_val_split(data, labels, train_size=0.9, val_size=0.1, random_state=42):\n",
    "#     \"\"\"\n",
    "#     Splits data into train and val sets with stratified sampling.\n",
    "#     Assumes test set has already been separated manually.\n",
    "#     \"\"\"\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     import pandas as pd\n",
    "\n",
    "#     assert abs(train_size + val_size - 1.0) < 1e-8, \"Train and val sizes must sum to 1.0\"\n",
    "\n",
    "#     data_by_class = {cls: data[labels == cls] for cls in labels.unique()}\n",
    "\n",
    "#     train_data, val_data = [], []\n",
    "#     train_labels, val_labels = [], []\n",
    "\n",
    "#     for cls, cls_data in data_by_class.items():\n",
    "\n",
    "#         # split each class into train and val\n",
    "#         cls_train, cls_val = train_test_split(\n",
    "#             cls_data,\n",
    "#             test_size=val_size,\n",
    "#             random_state=random_state\n",
    "#         )\n",
    "\n",
    "#         train_data.append(cls_train)\n",
    "#         val_data.append(cls_val)\n",
    "\n",
    "#         train_labels += [cls] * len(cls_train)\n",
    "#         val_labels += [cls] * len(cls_val)\n",
    "\n",
    "#     # combine\n",
    "#     train_data = pd.concat(train_data)\n",
    "#     val_data = pd.concat(val_data)\n",
    "\n",
    "#     # shuffle\n",
    "#     train_data = train_data.sample(frac=1, random_state=random_state)\n",
    "#     val_data = val_data.sample(frac=1, random_state=random_state)\n",
    "\n",
    "#     # align labels with shuffled indices\n",
    "#     train_labels = pd.Series(train_labels, index=train_data.index).reset_index(drop=True)\n",
    "#     val_labels = pd.Series(val_labels, index=val_data.index).reset_index(drop=True)\n",
    "\n",
    "#     # reset dataframe indices\n",
    "#     train_data = train_data.reset_index(drop=True)\n",
    "#     val_data = val_data.reset_index(drop=True)\n",
    "\n",
    "#     return train_data, val_data, train_labels, val_labels\n",
    "\n",
    "# train_data, val_data, train_labels, val_labels = stratified_train_val_split(train_val_set, train_val_set['class'])\n",
    "# print(\"Class Distribution in Training Data:\")\n",
    "# print(train_labels.value_counts(normalize=False))\n",
    "# print(\"Class Distribution in Validation Data:\")\n",
    "# print(val_labels.value_counts(normalize=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6090, 42)\n",
      "(338, 42)\n",
      "(339, 42)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.loc[:, ~train_data.columns.duplicated()]\n",
    "val_data = val_data.loc[:, ~val_data.columns.duplicated()]\n",
    "test_set = test_data.loc[:, ~test_data.columns.duplicated()]\n",
    "\n",
    "print(train_data.shape) \n",
    "print(val_data.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scaling = lambda x: np.log1p(x)\n",
    "stdscale = StandardScaler()\n",
    "\n",
    "# TRAIN\n",
    "time_T_tr = train_data['cryst_time']\n",
    "temp_T_tr = train_data['cryst_temp']\n",
    "# yield_tr = train_data['class']\n",
    "zeo_desc_tr = train_data.loc[:, 'deriv_dist_46':'nav_cm3_g']\n",
    "smiles_tr = train_data['osda_combined']\n",
    "zeo_code_tr = train_data['zeolite_code']\n",
    "\n",
    "# Fit scalers on train\n",
    "norm_time_tr = log_scaling(time_T_tr.values.reshape(-1, 1))\n",
    "norm_temp_tr = log_scaling(temp_T_tr.values.reshape(-1, 1))\n",
    "\n",
    "smiles_tr = np.array(smiles_tr).reshape(-1, 1)\n",
    "\n",
    "# Combine time and temperature\n",
    "# time_temp_tr_concat = np.hstack([norm_time_tr, norm_temp_tr])\n",
    "\n",
    "chemprop_feat_train = pd.DataFrame(\n",
    "    np.hstack([zeo_desc_tr]),\n",
    "    columns=list(zeo_desc_tr)\n",
    ")\n",
    "\n",
    "\n",
    "chemprop_input_train = pd.DataFrame(\n",
    "    np.hstack([smiles_tr, time_temp_tr_concat]),\n",
    "    columns=['osda_combined'] + ['norm_cryst_time', 'norm_cryst_temp']\n",
    ")\n",
    "\n",
    "# Validation\n",
    "\n",
    "time_T_v = val_data['cryst_time']\n",
    "temp_T_v = val_data['cryst_temp']\n",
    "# yield_v = val_data['class']\n",
    "zeo_desc_v = val_data.loc[:, 'deriv_dist_46':'nav_cm3_g']\n",
    "smiles_v = val_data['osda_combined']\n",
    "zeo_code_v = val_data['zeolite_code']\n",
    "\n",
    "\n",
    "norm_time_v = log_scaling(time_T_v.values.reshape(-1, 1))\n",
    "norm_temp_v = log_scaling(temp_T_v.values.reshape(-1, 1))\n",
    "# yield_v = np.array(yield_v).reshape(-1, 1)\n",
    "smiles_v = np.array(smiles_v).reshape(-1, 1)\n",
    "\n",
    "# Combine time and temperature\n",
    "# time_temp_v_concat = np.hstack([norm_time_v, norm_temp_v])\n",
    "\n",
    "chemprop_feat_val = pd.DataFrame(\n",
    "    np.hstack([zeo_desc_v]),\n",
    "    columns=list(zeo_desc_v)\n",
    ")\n",
    "\n",
    "chemprop_input_val = pd.DataFrame(\n",
    "    np.hstack([smiles_v, time_temp_v_concat]),\n",
    "    columns=['osda_combined', 'norm_cryst_time', 'norm_cryst_temp']\n",
    ")\n",
    "\n",
    "# Test\n",
    "time_T_te = test_data['cryst_time']\n",
    "temp_T_te = test_data['cryst_temp']\n",
    "# yield_te = test_data['class']\n",
    "zeo_desc_te = test_data.loc[:, 'deriv_dist_46':'nav_cm3_g']\n",
    "smiles_te = test_data['osda_combined']\n",
    "zeo_code_te = test_data['zeolite_code']\n",
    "\n",
    "norm_time_te = log_scaling(time_T_te.values.reshape(-1, 1))\n",
    "norm_temp_te = log_scaling(temp_T_te.values.reshape(-1, 1))\n",
    "# yield_te = np.array(yield_te).reshape(-1, 1)\n",
    "smiles_te = np.array(smiles_te).reshape(-1, 1)\n",
    "\n",
    "# Combine time and temperature\n",
    "# time_temp_te_concat = np.hstack([norm_time_te, norm_temp_te])\n",
    "\n",
    "chemprop_feat_test = pd.DataFrame(\n",
    "    np.hstack([zeo_desc_te]),\n",
    "    columns=list(zeo_desc_te)\n",
    ")\n",
    "\n",
    "chemprop_input_test = pd.DataFrame(\n",
    "    np.hstack([smiles_te, time_temp_te_concat]),\n",
    "    columns=['osda_combined', 'norm_cryst_time', 'norm_cryst_temp']\n",
    ")\n",
    "\n",
    "chemprop_input_train.to_csv('.../input_train.csv', index=False)\n",
    "chemprop_input_val.to_csv('.../input_val.csv', index=False)\n",
    "chemprop_input_test.to_csv('.../input_test.csv', index=False)\n",
    "\n",
    "zeo_code_tr.to_csv('.../zeo_code_train.csv', index=False)\n",
    "zeo_code_v.to_csv('.../zeo_code_val.csv', index=False)\n",
    "zeo_code_te.to_csv('.../zeo_code_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape:  (339, 36)\n"
     ]
    }
   ],
   "source": [
    "def drop_string_columns(df):\n",
    "    columns_to_drop = [col for col in df.columns if df[col].apply(lambda x: isinstance(x, str)).any()]\n",
    "    df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "    return df\n",
    "\n",
    "chemprop_feat_train_clean = drop_string_columns(chemprop_feat_train).fillna(0)\n",
    "chemprop_feat_val_clean = drop_string_columns(chemprop_feat_val).fillna(0)  \n",
    "chemprop_feat_test_clean = drop_string_columns(chemprop_feat_test).fillna(0)\n",
    "\n",
    "print(\"Test shape: \", chemprop_feat_test_clean.shape)\n",
    "\n",
    "chemprop_feat_train_clean.to_csv('.../feat_train.csv', index=False)\n",
    "chemprop_feat_val_clean.to_csv('.../feat_val.csv', index=False)\n",
    "chemprop_feat_test_clean.to_csv('.../feat_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
